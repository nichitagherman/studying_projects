{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import requests\n",
    "import random\n",
    "import ssl\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import tqdm\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подгрузка всех № участников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1 processed\n",
      "page 2 processed\n",
      "page 3 processed\n",
      "page 4 processed\n",
      "page 5 processed\n",
      "page 6 processed\n",
      "page 7 processed\n",
      "page 8 processed\n",
      "page 9 processed\n",
      "page 10 processed\n",
      "page 11 processed\n",
      "page 12 processed\n",
      "page 13 processed\n",
      "page 14 processed\n",
      "page 15 processed\n",
      "page 16 processed\n",
      "page 17 processed\n",
      "page 18 processed\n",
      "page 19 processed\n",
      "page 20 processed\n",
      "page 21 processed\n",
      "page 22 processed\n",
      "page 23 processed\n",
      "page 24 processed\n",
      "page 25 processed\n",
      "page 26 processed\n",
      "page 27 processed\n",
      "page 28 processed\n",
      "page 29 processed\n",
      "page 30 processed\n",
      "page 31 processed\n",
      "page 32 processed\n",
      "page 33 processed\n",
      "page 34 processed\n",
      "page 35 processed\n",
      "page 36 processed\n",
      "page 37 processed\n",
      "page 38 processed\n",
      "page 39 processed\n",
      "page 40 processed\n",
      "page 41 processed\n",
      "page 42 processed\n",
      "page 43 processed\n",
      "page 44 processed\n",
      "page 45 processed\n",
      "page 46 processed\n",
      "page 47 processed\n",
      "page 48 processed\n",
      "page 49 processed\n",
      "page 50 processed\n",
      "page 51 processed\n",
      "page 52 processed\n",
      "page 53 processed\n",
      "page 54 processed\n",
      "page 55 processed\n",
      "page 56 processed\n",
      "page 57 processed\n",
      "page 58 processed\n",
      "page 59 processed\n",
      "page 60 processed\n",
      "page 61 processed\n",
      "page 62 processed\n",
      "page 63 processed\n",
      "page 64 processed\n",
      "page 65 processed\n",
      "page 66 processed\n",
      "page 67 processed\n",
      "page 68 processed\n",
      "page 69 processed\n",
      "page 70 processed\n",
      "page 71 processed\n",
      "page 72 processed\n",
      "page 73 processed\n",
      "page 74 processed\n",
      "page 75 processed\n",
      "page 76 processed\n",
      "page 77 processed\n",
      "page 78 processed\n",
      "page 79 processed\n",
      "page 80 processed\n",
      "page 81 processed\n",
      "page 82 processed\n",
      "page 83 processed\n",
      "page 84 processed\n",
      "page 85 processed\n",
      "page 86 processed\n",
      "page 87 processed\n",
      "page 88 processed\n",
      "page 89 processed\n",
      "page 90 processed\n",
      "page 91 processed\n",
      "page 92 processed\n",
      "page 93 processed\n",
      "page 94 processed\n",
      "page 95 processed\n",
      "page 96 processed\n",
      "page 97 processed\n",
      "page 98 processed\n",
      "page 99 processed\n",
      "page 100 processed\n",
      "page 101 processed\n",
      "page 102 processed\n",
      "page 103 processed\n",
      "page 104 processed\n",
      "page 105 processed\n",
      "page 106 processed\n",
      "page 107 processed\n",
      "page 108 processed\n",
      "page 109 processed\n",
      "page 110 processed\n",
      "page 111 processed\n",
      "page 112 processed\n",
      "page 113 processed\n",
      "page 114 processed\n",
      "page 115 processed\n",
      "page 116 processed\n",
      "page 117 processed\n",
      "page 118 processed\n",
      "page 119 processed\n",
      "page 120 processed\n",
      "page 121 processed\n",
      "page 122 processed\n",
      "page 123 processed\n",
      "page 124 processed\n",
      "page 125 processed\n",
      "page 126 processed\n",
      "page 127 processed\n",
      "page 128 processed\n",
      "page 129 processed\n",
      "page 130 processed\n",
      "page 131 processed\n",
      "page 132 processed\n",
      "page 133 processed\n",
      "page 134 processed\n",
      "page 135 processed\n",
      "page 136 processed\n",
      "page 137 processed\n",
      "page 138 processed\n",
      "page 139 processed\n",
      "page 140 processed\n",
      "page 141 processed\n",
      "page 142 processed\n",
      "page 143 processed\n",
      "page 144 processed\n",
      "page 145 processed\n",
      "page 146 processed\n",
      "page 147 processed\n",
      "page 148 processed\n",
      "page 149 processed\n",
      "page 150 processed\n",
      "page 151 processed\n",
      "page 152 processed\n",
      "page 153 processed\n",
      "page 154 processed\n",
      "page 155 processed\n",
      "page 156 processed\n",
      "page 157 processed\n",
      "page 158 processed\n",
      "page 159 processed\n",
      "page 160 processed\n",
      "page 161 processed\n",
      "page 162 processed\n",
      "page 163 processed\n",
      "page 164 processed\n",
      "page 165 processed\n",
      "page 166 processed\n",
      "page 167 processed\n",
      "page 168 processed\n",
      "page 169 processed\n",
      "page 170 processed\n",
      "page 171 processed\n",
      "page 172 processed\n",
      "page 173 processed\n",
      "page 174 processed\n",
      "page 175 processed\n",
      "page 176 processed\n",
      "page 177 processed\n",
      "page 178 processed\n",
      "page 179 processed\n",
      "page 180 processed\n",
      "page 181 processed\n",
      "page 182 processed\n",
      "page 183 processed\n",
      "page 184 processed\n",
      "page 185 processed\n",
      "page 186 processed\n",
      "page 187 processed\n",
      "page 188 processed\n",
      "page 189 processed\n",
      "page 190 processed\n",
      "page 191 processed\n",
      "page 192 processed\n",
      "page 193 processed\n",
      "page 194 processed\n",
      "page 195 processed\n",
      "page 196 processed\n",
      "page 197 processed\n",
      "page 198 processed\n",
      "page 199 processed\n",
      "CPU times: user 24min 31s, sys: 28.7 s, total: 25min\n",
      "Wall time: 34min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Позволяет обращаться к сайту без верификации безопасности. В противном случае не допускает к Казахскому сайту\n",
    "context = ssl._create_unverified_context()\n",
    "# Итерируемся по каждому page с № участников, пока не закончатся page\n",
    "error_signal = None\n",
    "page = 1\n",
    "df_info = pd.DataFrame()\n",
    "while error_signal is None:\n",
    "    try:\n",
    "        # Будем вытаскивать по-максимуму инфы с одной страницы, какую только позволяет сайт: по 2000 участников\n",
    "        url = 'https://goszakup.gov.kz/ru/registry/supplierreg?count_record=2000&page=' + str(page)\n",
    "        web_page = urllib.request.urlopen(url, context = context)\n",
    "        # Распарсим информацию с сайта с помощью BeautifulSoup\n",
    "        soup = BeautifulSoup(web_page, \"html.parser\", from_encoding = 'utf-8')\n",
    "        table_find = soup.find('table', attrs={'class':'table table-bordered table-striped dataTable responsive'})\n",
    "        all_td = table_find.findAll('td')\n",
    "        for ind, val in enumerate(all_td):\n",
    "            # Каждый пятый элемент в all_td содержит № участника: как раз то, что нам надо вытащить\n",
    "            if ind % 5 == 0:\n",
    "                # Используем strip, чтобы удалить все пробельные символы в строке. Номер участников будем сохранять в колонке id\n",
    "                df_info = df_info.append({'id': val.text.strip()}, ignore_index = True)\n",
    "        print('page {} processed'.format(page))\n",
    "        page += 1\n",
    "    except:\n",
    "        error_signal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294402, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294402, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение результата\n",
    "df_info.to_csv('kazakhstan_all_participants_numbers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выгрузка данных о участниках"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ребята решили не запариваться, поэтому что для поставщиков, что для заказчиков сделали одинаковую страницу show_supplier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv('kazakhstan_all_participants_numbers.csv', dtype = str).drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем только недостающие данные в последней версии файла data_main_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_main_info = pd.read_csv('main_Kazakhstan.csv', dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_add = list(set(df_info['id']) - set(data_main_info['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Функция для парсинга информации о поставщике по его id на сайте\n",
    "def parse_supplier_data(participant_id):\n",
    "    while True:\n",
    "        try:\n",
    "            # List of proxy servers from http://spys.one/en/. All ips were checked on fast response prior\n",
    "            # to main function run. IMPORTANT: TAKE HTTPS SERVERS ONLY, NOT HTTP\n",
    "            proxy_list = ['157.245.4.19:3128',\n",
    "             '188.126.45.57:3128', '94.141.60.69:8080', '5.16.13.38:8080',\n",
    "              '46.73.33.253:8080', '93.175.203.124:56925',\n",
    "              '35.177.127.142:3128', '167.172.167.174:8080', '212.92.204.54:80',\n",
    "              '86.110.27.165:3128', '62.173.145.48:3128', '87.103.234.116:3128',\n",
    "              '80.249.82.44:32004', '86.57.177.8:31022', '181.191.243.110:8080']\n",
    "            \n",
    "            url = 'https://goszakup.gov.kz/ru/registry/show_supplier/' + participant_id\n",
    "            proxy = {'https': random.choice(proxy_list)}\n",
    "            # If response is not given within 5 seconds, break: timeout=5\n",
    "            web_page = requests.get(url, proxies=proxy, verify=False, timeout=5)\n",
    "            # web_page = requests.get(url, verify=False, timeout=5)\n",
    "            soup = BeautifulSoup(web_page.text, \"html.parser\")\n",
    "            # print(participant_id)\n",
    "            # Вся релевантная информация содержится в таблицах с классом: table table-striped\n",
    "            all_tables = soup.findAll('table', attrs={'class':'table table-striped'})\n",
    "\n",
    "            if len(all_tables) == 0:\n",
    "                continue\n",
    "\n",
    "            # Здесь распарсим только первые 3 раздела. Последний раздел \"контактная информация\" сохраним в отдельный \n",
    "            # info_contact_info, так как она содержит несколько, заранее не определенное количество строк\n",
    "            info_main = {}\n",
    "            for ind_table, table in enumerate(all_tables[0:(len(all_tables)-1)]):\n",
    "                # В каждой таблице несколько строк: tr\n",
    "                all_tr = table.findAll('tr')\n",
    "                # Учтем номер таблицы, чтобы правильно задать названия колонок по разделам\n",
    "                if ind_table == 0:\n",
    "                    # В каждой строке два столбца: th и td: th -- название инфы, td -- сама инфа\n",
    "                    for tr in all_tr:\n",
    "                        attr = tr.find('th')\n",
    "                        val = tr.find('td')\n",
    "                        info_main.update({'Основное.' + attr.text.strip(): val.text.strip()})\n",
    "                elif ind_table == 1:\n",
    "                    # В каждой строке два столбца: th и td: th -- название инфы, td -- сама инфа\n",
    "                    for tr in all_tr:\n",
    "                        attr = tr.find('th')\n",
    "                        val = tr.find('td')\n",
    "                        info_main.update({'Атрибуты.' + attr.text.strip(): val.text.strip()})\n",
    "                # Бывает, что отсутствует информация о руководителе. В таком случае ind_table < 2. Закептчим этот случай\n",
    "                elif ind_table == 2:\n",
    "                    # В каждой строке два столбца: th и td: th -- название инфы, td -- сама инфа\n",
    "                    for tr in all_tr:\n",
    "                        attr = tr.find('th')\n",
    "                        val = tr.find('td')\n",
    "                        info_main.update({'Руководитель.' + attr.text.strip(): val.text.strip()})\n",
    "            # Добавим, наконец, информацию о id\n",
    "            info_main.update({'id': participant_id})\n",
    "            \n",
    "\n",
    "#             # Здесь распарсим только раздел \"Контактная информация\". Вся инфа сидит в tr со второго по последний\n",
    "#             all_tr = all_tables[(len(all_tables)-1)].findAll('tr')\n",
    "#             info_contact_info = []\n",
    "#             # Бывает, что контактная информация отсутствует. Словим эти случаи\n",
    "#             if len(all_tr) > 1:\n",
    "#                 for tr in all_tr[1:len(all_tr)]:\n",
    "#                     # Вся информация содержится в td. Вытащим ее\n",
    "#                     all_td = tr.findAll('td')\n",
    "#                     # Сохраним все в df_info_contact_info под соответствующие колонки\n",
    "#                     info_contact_info.append({'id': participant_id,\n",
    "#                                             'Страна': all_td[0].text.strip(),\n",
    "#                                             'КАТО': all_td[1].text.strip(),\n",
    "#                                             'Полный адрес(рус)': all_td[2].text.strip(),\n",
    "#                                             'Полный адрес(каз)': all_td[3].text.strip(),\n",
    "#                                             'Тип адреса': all_td[4].text.strip()})\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "#             continue\n",
    "            print(\"Sleep\")\n",
    "            time.sleep(20)\n",
    "        break\n",
    "    return info_main\n",
    "#     return [info_main, info_contact_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unparalleled version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18078/18078 [5:01:59<00:00,  2.28it/s]   \n"
     ]
    }
   ],
   "source": [
    "result_add_info = []\n",
    "for add_info in tqdm.tqdm(enumerate(ids_add), total=len(ids_add)):\n",
    "    result_add_info.append(parse_supplier_data(add_info[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_main_info = data_main_info.append(result_add_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_main_info.to_csv('main_Kazakhstan_new.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paralleled version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of suppliers ids\n",
    "participant_ids = df_info['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize 30 processes\n",
    "p = Pool(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 74433/74434 [2:41:31<00:00,  7.68it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Use multiprocessing to parse data\n",
    "result_main_info = []\n",
    "# result_contact_info = []\n",
    "# for info_main, info_contact_info in tqdm.tqdm(p.imap(parse_supplier_data, participant_ids),\n",
    "#                                               total=len(participant_ids)):\n",
    "#     result_main_info.append(info_main)\n",
    "#     result_contact_info.append(info_contact_info)\n",
    "\n",
    "for info_main in tqdm.tqdm(p.imap(parse_supplier_data, participant_ids),\n",
    "                                              total=len(participant_ids)):\n",
    "    result_main_info.append(info_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.close()\n",
    "time.sleep(15)\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data_main_info in DataFrame object\n",
    "data_main_info = pd.DataFrame([x for x in result_main_info])\n",
    "# Flatten result_contact_info\n",
    "# flattened_list = [y for info in result_contact_info for y in info]\n",
    "# data_contact_info = pd.DataFrame(flattened_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276342, 26)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_main_info.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_main_info.to_csv('main_Kazakhstan.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_contact_info.to_csv('contact_Kazakhstan.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
